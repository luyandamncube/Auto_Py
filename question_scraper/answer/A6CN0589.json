{"question": "You manage a process that performs analysis of daily web traffic logs on an HDInsight cluster. Each of the 250 web servers generates approximately 10 megabytes(MB) of log data each day. All log data is stored in a single folder in Microsoft Azure Data Lake Storage Gen 2.You need to improve the performance of the process.Which two changes should you make? Each correct answer presents a complete solution.NOTE: Each correct selection is worth one point.\nA.Combine the daily log files for all servers into one file\nB.Increase the value of the mapreduce.map.memory parameter\nC.Move the log files into folders so that each day\u2122s logs are in their own folder\nD.Increase the number of worker nodes\nE.Increase the value of the hive.tez.container.size parameter", "answer": "AC", "description": "A: Typically, analytics engines such as HDInsight and Azure Data Lake Analytics have a per-file overhead. If you store your data as many small files, this cannegatively affect performance. In general, organize your data into larger sized files for better performance (256MB to 100GB in size). Some engines andapplications might have trouble efficiently processing files that are greater than 100GB in size.C: For Hive workloads, partition pruning of time-series data can help some queries read only a subset of the data which improves performance.Those pipelines that ingest time-series data, often place their files with a very structured naming for files and folders. Below is a very common example we see fordata that is structured by date:\\DataSet\\YYYY\\MM\\DD\\datafile_YYYY_MM_DD.tsvNotice that the datetime information appears both as folders and in the filename.References:https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance"}