{"question": "You are developing a data engineering solution for a company. The solution will store a large set of key-value pair data by using Microsoft Azure Cosmos \nDB.The solution has the following requirements:Data must be partitioned into multiple containers.Data containers must be configured separately.Data must be accessible from applications hosted around the world.The solution must minimize latency.\nYou need to provision Azure Cosmos DB.\nA.Cosmos account-level throughput.\nB.Provision an Azure Cosmos DB account with the Azure Table API. Enable geo-redundancy.\nC.Configure table-level throughput.\nD.Replicate the data globally by manually adding regions to the Azure Cosmos DB account.\nE.Provision an Azure Cosmos DB account with the Azure Table API. Enable multi-region writes.", "answer": "E", "description": "Scale read and write throughput globally. You can enable every region to be writable and elastically scale reads and writes all around the world. The throughput thatyour application configures on an Azure Cosmos database or a container is guaranteed to be delivered across all regions associated with your Azure Cosmosaccount. The provisioned throughput is guaranteed up by financially backed SLAs.References:https://docs.microsoft.com/en-us/azure/cosmos-db/distribute-data-globally"}