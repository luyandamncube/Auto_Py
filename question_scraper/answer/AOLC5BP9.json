{"question": "Each day, company plans to store hundreds of files in Azure Blob Storage and Azure Data Lake Storage. The company uses the parquet format.You must develop a pipeline that meets the following requirements:Process data every six hoursOffer interactive data analysis capabilitiesOffer the ability to process data using solid-state drive (SSD) cachingUse Directed Acyclic Graph(DAG) processing mechanismsProvide support for REST API calls to monitor processesProvide native support for PythonIntegrate with Microsoft Power BIYou need to select the appropriate data technology to implement the pipeline.Which data technology should you implement?\nA.Azure SQL Data Warehouse\nB.HDInsight Apache Storm cluster\nC.Azure Stream Analytics\nD.HDInsight Apache Hadoop cluster using MapReduce\nE.HDInsight Spark cluster", "answer": "B", "description": "Storm runs topologies instead of the Apache Hadoop MapReduce jobs that you might be familiar with. Storm topologies are composed of multiple components thatare arranged in a directed acyclic graph (DAG). Data flows between the components in the graph. Each component consumes one or more data streams, and canoptionally emit one or more streams.Python can be used to develop Storm components.References:https://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview"}